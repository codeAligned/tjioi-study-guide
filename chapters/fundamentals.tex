\chapter{Fundamentals}

\section{Algorithms}

In the field of computer science, programmers use \textbf{algorithms} to solve complicated problems. What is an algorithm? An algorithm is a procedure, or series of problem-solving operations, in order to accomplish a certain task. Algorithms can range from simple operations, such as finding the smallest value in an array, to incredibly complex jobs, such as Facebook's facial recognition or Google's PageRank.

\begin{algorithm}[H]
\caption{Finding the Maximum Value in an Array}
\begin{algorithmic}
\State $M \gets 0$
\ForAll{$a_i$ in $A$} \Comment iterate through elements of $A$
    \If{$a_i > M$}
        \State $M \gets a_i$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

For example, here is an algorithm to find the largest value in an array.  We assign a value of $ 0 $ to $ M $, and then \textbf{iterate} over the contents of the array. Iteration is a computer science term, meaning we examine each element of the array one at a time. While we iterate over the array, we check each value against our existing maximum value.  If the value is greater than the largest value we've already found, then we will update our maximum to reflect that value.  Once we have iterated over all elements in the array, we are guaranteed to have found the largest value in the array.

Note that this algorithm was not written in a formal programming language above, nor was it described in a programming language.  This is because an algorithm is an idea, not lines of code.  It is important to remember that before one writes code to accomplish a task, one first thinks through the problem, and comes up with a solution in words rather than code.  The language that one uses is merely the medium of communication to the computer: just as one can express the same ideas in English and French, an algorithm should be able to be implemented in Java, C/C++, Python, or any language.

\section{Computational Complexity}

Remember when we talked about how algorithms can range from simple to complex? How can we tell the difference between a simple algorithm and a complicated one? The answer lies in \textbf{Computational Complexity}.  Computational Complexity is a measure of how efficiently a program will run.  Although computers appear to be instant, all operations take time.  Computers are limited to a finite, yet extremely large, number of operations per second. In most modern computers, approximately $ 10^8 $ operations can be done per second.  This seems like a huge number, but in practice, we find that we quickly use it up, especially when dealing with large amounts of data.  This is why it is incredibly important to write \textbf{efficient} algorithms - a poorly written algorithm that solves a task in three days of computation time could be completed in under a second if written efficiently! Thus far, we have only developed a qualitative idea of efficiency, but computer scientists want rigor, and thus use \textbf{Big-O notation} in order to quantify the efficiency of their algorithms.

\subsection{Big-O Notation}

Big-O notation is the computer scientists way of describing the efficiency of an algorithm.  Think back to our first algorithm (finding the maximum in a list).  We examine each element of the array, one at a time.  Thus, if our array is of length $ n $, we will need $ n $ operations to complete our algorithm.  This is true regardless of the value of $ n $, so the operation is said to be $ O(n) $.  But what about the initial assignment? Shouldn't the algorithm be $ O(n + 1) $?  In computer science, we care about the efficiency when the data sets are \textbf{large}, and as $ n $ becomes large, the number of operations will grow at the same rate that $ n $ grows, and the $ 1 $ will become negligible.  We also only care about the \textbf{order} of the growth, that is, $ O(cn) $ is the same thing as $ O(n) $, where $ c $ is any constant factor.

An algorithm grows only as fast as its slowest operation.  For example, a nested for-loop will be $ O(n^2) $, and so another for-loop afterwards with $ O(n) $ efficiency will be negligible compared to the nested loop.  Note that an algorithm with $ O(n^2) $ efficiency is not guaranteed to run slower than one of $ O(n) $, as the faster operations may have constant terms associated with them that are very large.  However, we know for certain that for sufficiently large data set size, the $ O(n^2) $ algorithm will be slower.

Other common runtimes include $ O(\log n) $ or $ O(n\log n) $.  A good question to ask may be, "what is the base of the logarithm in question?"  The answer is that it does not matter - the difference between two logarithms in different bases is merely a constant factor, which we disregard in our complexity analysis.

In a more theoretical sense, one can determine the Big-O efficiency of an algorithm mathematically.  Given an algorithm, we can write some function, which takes the size of the data as input, and outputs the number of operations.  For example, consider an algorithm which loops over an array twice, then, for \textbf{each} element of the array, loops over the \textbf{entire} array three times, then loops over the first three terms of the array.  Then, assuming the length of our array is given by $ n $ (where $ n > 3 $), our function is given by $ f(n) = 2n + 3n^2 + 3 $.  We now define the Big-O efficiency of the function, $ O(g(n)) $, to be a function satisfying:

$$ \frac{|f(n)|}{|g(n)|} = M $$

\noindent where $ M $ is a finite, positive, non-zero value.  In other words, $ f(n) $ grows on the order of $ g(n) $.  Choosing $ g(n) = n^2 $ will result in a value of $ 3 $ for M, and so the function $ f(n) $ and the associated algorithm is said to be of order $ O(n^2) $.  Note that this choice of $ g(n) $ is not unique, but is the "simplest" choice.  This process is similar to a leading term test, often used when taking the limit of a rational function in BC Calculus, in which the term that grows fastest "dominates" the rest as the independent variable becomes large.

As an example, consider the following algorithm:

\begin{algorithm}[H]
\caption{Finding Pairs}
\begin{algorithmic}
\State $ X \gets 0 $
\ForAll{$a_i$ in $A$}
    \ForAll{$a_j$ in $A$ and $ j > i $}
        \If{$a_i = a_j$}
            \State $X \gets X + 1$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

What is the efficiency of the algorithm?  We can see that there is one loop that iterates over the entire array $ A $, and another loop that iterates over all the elements after the first element chosen.  If both loops were over the entire array, then it would clearly be $ O(n^2) $, but in this case, the inner loop does not iterate over the entire array each time!  From this analysis, we can see that the efficiency of the program seems to be faster, as looping over the entire array in both loops means that we compare every pair twice, as for any $ i $ and $ j $, we will check $ i = j $ and $ j = i $, but looping the way we do in this algorithm ensures we check each pair only once.  This means that our program only needs half the number of operations as an $ O(n^2) $ algorithm!  Unfortunately, as we have previously discussed, when using Big-O analysis, we discard constant factors and only care about the order of the growth.  Despite only needing $ \frac{1}{2}n^2 $ operations, the number of operations still grows as $n^2$.  Thus, this algorithm is also $ O(n^2) $.

\subsection{Program Speed}

One of the largest differences between different programming languages is that some are faster than others.  For example, compiled languages like C and C++ are significantly faster than languages like Python.  Usually, contests will allow for different time limits based on the language used, but it is often best to choose a fast language as the increase in speed will more than compensate the difference in time allotted.

Another thing to keep in mind is that most problems have an intended solution, and the size of the test cases will reflect the efficiency of the intended solution.  The following are standard guidelines for time complexity (assuming 1 second of runtime):

\begin{itemize}
    \item $ n \leq 10$: $ O(n!) $
    \item $ n \leq 25$: $ O(2^n) $
    \item $ n \leq 50$: $ O(n^4) $
    \item $ n \leq 500$: $ O(n^3) $
    \item $ n \leq 5000$: $ O(n^2) $
    \item $ n \leq 100000$: $ O(n\log n) $
    \item $ n \leq 1000000$: $ O(n) $
\end{itemize}

When given a problem, by looking at the size of the test cases, we can infer information about what kind of algorithm to use.  For example, consider the following problem:

\begin{Problem}
Devon has $ N $ cookies ($ 1 \leq N \leq 6000 $), each with between $ 0 $ and $ M $ chocolate chips ($ 1 \leq M \leq 1000000 $).  Devon would like to put these cookies in order, from most to least chocolate chips.  Please help Devon do so!
\end{Problem}

Without solving the problem, we can guess that the intended solution will likely be $ O(n^2) $, which will give us some information regarding how we should approach this problem.  We know that this is the case as the number of cookies is bounded by $ 6000 $, which is approximately the limit for an $ O(n^2) $ solution.  Note that the number of chocolate chips does not matter, as we are only using that number as a tool for comparison.  Because we have bounded the solution to be $ O(n^2) $, we know that we are able to afford a nested for-loop to solve our problem (a runtime of $ O(n^2) $ is generally an indicator of a nested for-loop, as we run $ n $ operations $ n $ times each, for a total of $ n^2 $).

What if the number of cookies was bounded by $ 1 \leq N \leq 100000 $?  In that case, we must find a faster solution - an $ O(n^2) $ solution will no longer be sufficient! In this case, we will need an $ O(n\log n) $ solution.  A logarithm in the runtime usually denotes a recursive or binary search solution, topics that we will discuss later in this guide.  The correct solution for this problem is to use a sort, and there are many different ways to accomplish this task.  We will discuss sorts more expansively, as well as the runtimes, advantages, and disadvantages of each sort, in a later section.
