\chapter{Graph Theory}

In Chapter 2, we discussed how we can represent the search space of a recursive, brute-force algorithm as a tree of states. This means that all recursive, brute-force algorithms basically perform the same thing: visit each child state and recur. In other words, we learned one way in which we can attack a specific problem using a nonspecific approach. In fact, many seemingly specific problems can be approached by using a variant of a generic algorithm. For these reasons, it is often useful to abstract specific problems into more general, mathematical objects, such as graphs. 


\section{Definitions and terminology}

Formally, a \textbf{graph} consists of a set of \textbf{vertices}, or \textbf{nodes}, and a set of \textbf{edges}, where each edge connects two vertices. An \textbf{undirected} graph is one in which every edge is bidirectional. In other words, if an edge connects vertex $A$ to vertex $B$, then it also connects $B$ to $A$. In contrast, an \textbf{directed} graph is one in which every edge points in a single direction. Directed graphs are also commonly called \textbf{digraphs}. Note, however, that it is possible for two directed edges to exist between two vertices. In a \textbf{weighted} graph, each edge is associated with some weight.

Graphs can take on many unusual structures. For example, we may sometimes allow edges to point from a vertex to itself, known as a \textit{self-loop}. Graphs may also contain multiple edges between the same pair of vertices. For the sake of simplicity, we will not deal with graphs that contain self-loops and multiple edges in this chapter. This is because these cases usually serve little purpose in solving actual problems. 

A \textbf{path} is a sequence of edges which connected a sequence of vertices.  If the first and last vertices in the sequence are the same, then we refer to the path as a \textbf{cycle}. For example, in figure~\ref{digraph-example}, $(E, F, C, H, C, G)$ is a valid path and $(H, E, F, A)$ is a valid cycle. Two vertices are said to be \textit{connected} if these exists a path between them. A connected graph is a graph in which all pairs of vertices are connected. 

A \textbf{tree} is a special kind of undirected graph that contains no cycles. Trees are particularly useful because we can choose a single node to be the \textbf{root} of the tree. Then all edges in the tree are given a natural direction, namely towards the root.

\begin{figure}
\centering
\begin{minipage}{0.48\linewidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=60mm/#1}]
\draw (0, 0) node [vertex] (n1) {A};
\draw (2, 0) node [vertex] (n2) {B};
\draw (4, 0) node [vertex] (n3) {C};
\draw (6, 0) node [vertex] (n4) {D};
\draw (0, 2) node [vertex] (m1) {E};
\draw (2, 2) node [vertex] (m2) {F};
\draw (4, 2) node [vertex] (m3) {G};
\draw (6, 2) node [vertex] (m4) {H};
\draw[->] (m1) -- (m2);
\draw[->] (m2) -- (n1);
\draw[->] (n1) -- (m1);
\draw[->] (n2) edge [bend left] (m2);
\draw[->] (m2) edge [bend left] (n2);
\draw[->] (n2) -- (n3);
\draw[->] (m2) -- (m3);
\draw[->] (m2) -- (n3);
\draw[->] (n3) -- (m3);
\draw[->] (n3) edge [bend left] (m4);
\draw[->] (m4) edge [bend left] (n3);
\draw[->] (n4) -- (m4);
\end{tikzpicture}
\end{minipage}
\label{digraph-example}
\caption{An example of a digraph.}
\end{figure}


\section{Representations of graphs}

Before discussing graph algorithms, it is important to understand how to represent a graph so that we have access information efficiently. In this section, we will go over the two main graph representations and their respective advantages.

\subsection{Adjacency matrices}

For a graph with $V$ vertices, an \textit{adjacency matrix} is an $V \times V$ matrix $A$, where $A_{ij}$ (the entry in row $i$ and column $j$) represents the edge weight between vertices $i$ and $j$. It no edge exists between $i$ and $j$, then $A_{ij} = 0$. In an unweighted graph, we usually represent all of the edge weights as $1$. Note that in an undirected graph, $A$ would be symmetric about its main diagonal. Below is the adjacency matrix for the graph in figure~\ref{digraph-example}.

\begin{center}
\begin{minipage}{0.45\linewidth}
\begin{tabular}{c c c c c c c c c}
         & A & B & C & D & E & F & G & H \\
A        & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
B        & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
C        & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
D        & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
E        & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
F        & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\
G        & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
H        & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}
\end{minipage}
\end{center}

An adjacency matrix allows us to determine the edge weight or existence of an edge between two vertices in $O(1)$. Furthermore, we can get all neighbors of any vertex in $O(V)$ by looping across a row of the adjacency matrix. However, an adjacency matrix requires $O(V^2)$ space, which may be undesirable if the number of vertices is large (say, around $V = 2000$). This is especially true when the graph is \textit{sparse}, or as relatively few edges. 

\subsection{Adjacency lists}

Alternatively, we can represent a graph as a list of sets, where each set contains the neighbors of an associated vertex. This is known as an \textbf{adjacency list}. In an weighted graph, the sets contains pairs of vertices and their corresponding edge weights. Adjacency lists are less straightforward to implement than adjacency matrices; they are usually implemented as an array of dynamically allocated lists (see \ref{sec:dynamic-array}). Below is an adjacency list for the graph shown above. 

\begin{center}
\begin{minipage}{0.45\linewidth}
\begin{tabular}{ c|c c c c}
A        & E \\
B        & C & F \\
C        & G & H \\
D        & H \\
E        & F \\
F        & A & B & C & G \\
G        & \\
H        & C \\
\end{tabular}
\end{minipage}
\end{center}

An adjacency list usually scales better for large $N$ than an adjacency matrix because it requires $O(V+E)$ space complexity. In the worst case, we can also access all neighbors of any vertex in $O(V)$. The main disadvantage of an adjacency list, however, is that we cannot find the edge between any two vertices in $O(1)$. Fortunately, many algorithms do not require constant-time edge look-up, making adjacency lists an excellent choice for balancing both time and space complexity. 


\section{Graph search algorithms}

In this section, we will apply what we've just discussed to learn a number of well-known graph algorithms. 


\subsection{Search trees}

In section~\ref{recursive-search}, we talked about using recursion to explore states, specifically in the case of generating permutations. We also represented those states as a search tree.

In what order did we traverse that tree? We started at the root node, or level 0. Then we chose a node at level 1, and then another at level 2, and so on until we reached the bottom of the tree. At that point, we've reached one node on each of the levels, but no more. Only after that will we start to check other nodes on the same levels.

Why does our recursive method behave like this? Let's say that every time we make a recursive call, we're visiting a node. So when we visit a permutation X, if we are able to we immediately generate a new permutation, we do so, and we visit the new permutation through a recursive call. Only after we get back to the original call do we generate other permutations. Thus, our use of recursion ensures that we examine the deeper elements first, which is why we call this \textit{depth first search}.


\subsection{Depth-first search}

We can use this same recursive strategy to traverse all nodes in a graph, by treating each node as a possible state and recurring on each of its neighbors.

One change that we must make, however, is that we must check whether we've already visited a node before recurring on it. This is because unlike trees, graphs can contain cycles; consequently, there may be multiple ways to reach any given node. If we don't do this, we might recurse indefinitely along a cycle.

The only difference between an \textit{undirected graph} and a \textit{tree} is that a tree contains no cycles. In other words, it is \textbf{acyclic}. We can express any acyclic undirected graph as a tree if we pick an arbitrary node to be the root, then treat its neighbors as its children, and so on.

We can apply depth-first search to a graph represented by an adjacency list, which stores the neighbors of each node. We can also apply this to a grid, in which the neighbors of a cell are the four adjacent cells. Interestingly, the edges that were traversed in the graph correspond to the search tree of the recursion. Assuming the graph is connected, the search tree is a spanning tree of the graph, which means that it includes all of the vertices in the graph.

\begin{algorithm}[H]
\caption{Graph traversal with depth-first search}
\begin{algorithmic}

\State $visited \gets$ empty set
\Function{DFS}{v}
    \State add $v$ to $visited$
    \For{$w$ in $v.neighbors$}
        \If{$w$ not in $visited$}
            \State \Call{DFS}{$w$}
        \EndIf
    \EndFor
\EndFunction

\end{algorithmic}
\end{algorithm}



\subsection{Breadth-first search}

Another way to explore the tree is to go level by level, where level $k$ of a tree represents all nodes with depth $k$. We start at the root node at level 0. Next, we examine its children on level 1. Then, we examine the children of the nodes on level 1, which are on level 2, and so on. Similarly, we can explore a graph using the same strategy, if we ignore repeated nodes, just as we did for depth first search. How can we implement this in our code?

% maybe math notation is not that good
We could start by forming a list $A_0$ of nodes on level 0; i.e., only the root node. Then, we can find $A_1$ by adding all the children of the nodes in $A_0$, and so on, each time generating $A_{k+1}$ from $A_k$.

In fact, we can do this with only one array, $Q = A_0 + A_1 + ... + A_n$. While we're processing $A_k$, we can simply append the elements of $A_k$ to the end of the list. What does this remind us of? That's right, a queue! For each node we remove from the front of the queue, we insert the children of that node into the back of the queue. This ensures that we process all nodes in the graph in breadth-first order.

\begin{algorithm}[H]
\caption{Graph traversal with breadth-first search}
\begin{algorithmic}

\Function{BFS}{$v_0$}
    \State $visited \gets$ empty set
    \State $queue \gets \{v_0\}$

    \While{$queue$ is not empty}
        \State remove $v$ from queue
        \For{$w$ in $v.neighbors$}
            \If{$w$ not in $visited$}
                \State add $w$ to queue
            \EndIf
        \EndFor
    \EndWhile
\EndFunction

\end{algorithmic}
\end{algorithm}


\section{Shortest paths}

The shortest-paths problem appears frequently in competitive programming. Given two nodes $u$ and $v$ in a directed graph, we want to find a path between $u$ and $v$ such that the sum of the edge weights of the path is minimized.

\begin{center}
\begin{tikzpicture}[very thick,edge from parent/.style={draw,<-},level/.style={sibling distance=30mm/#1},scale=0.8]
\draw (0, 1) node [vertex] (v1) {1};
\draw (2.5, 2) node [vertex] (v2) {2};
\draw (4, -1) node [vertex] (v3) {3};
\draw (6, 2) node [vertex] (v4) {4};
\draw (8, 0) node [vertex] (v5) {5};
\draw[->] (v1) -- (v2) node[midway, above left] {4};
\draw[->] (v2) -- (v3) node[midway, above right] {2};
\draw[->] (v1) -- (v3) node[midway, below] {7};
\draw[->] (v2) -- (v4) node[midway, above] {6};
\draw[->] (v3) -- (v4) node[midway, right] {3};
\draw[->] (v3) -- (v5) node[midway, below] {5};
\draw[->] (v4) -- (v5) node[midway, above right] {4};
\end{tikzpicture}
\end{center}


\subsection{Floyd-Warshall algorithm}

The Floyd-Warshall algorithm solves the multi-source shortest paths problem; that is, it solves the shortest path problem for every pair of vertices. We use a matrix of distances $dist$, which stores the shortest distance we have found so far for each pair of vertices.

Then, for every vertex $k$, and every pair of vertices $i \rightarrow j$, we try to see if $dist(i,j)$ can be improved by going through $k$. In other words, if the current best distance from $i \rightarrow k \rightarrow j$ is shorter than the current best distance for $i \rightarrow j$ (which could be $\infty$). If it is, we update $dist(i,j)$, which we want to be as short as possible. This is similar to the \textit{relax} operation that we will use for Dijkstra's algorithm, though it may not necessarily involve edges in the original graph.

\begin{algorithm}[H]
\caption{Floyd-Warshall}
\begin{algorithmic}
\State $dist(i, j) \gets \infty$ for vertices $i, j$
\ForAll{vertices $i$}
    \State $dist(i,i) \gets 0$
\EndFor
\ForAll{edges $(u,v)$}
	\State $dist(u,v) \gets weight(u,v)$
\EndFor
\ForAll{vertices $k$}
	\ForAll{vertices $i$}
    	\ForAll{vertices $j$}
        	\If{$dist(i,j) > dist(i,k)+dist(k,j)$}
            	\State $dist(i,j) \gets dist(i,k)+dist(k,j)$
            \EndIf
        \EndFor
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

One way to see why this works is to consider two vertices $u$ and $v$ for which we already know the shortest path from $u \rightarrow v$. Once we consider each of the vertices in that shortest path, $dist(u,v)$ will be the length of the shortest path.

\subsection{Dijkstra's algorithm}
Dijkstra's algorithm calculates the shortest path from a source vertex $u$ to every other node in the graph. It functions using a greedy approach, continually removing the closest unvisited node to $u$, and then relaxing the edges connected to it. We can implement this by placing our nodes in a \textbf{priority queue}, which is a queue that allows us to efficiently remove the element with the minimum value. If we're only interested in the shortest path from $u \rightarrow v$, we can stop once we remove $v$ from the priority queue.

But why does it work? Every time we remove a vertex $u$ from $pq$, $dist(u)$ is monotonically increasing. This is because the priority queue always gives the minimum element, and for any $u$ removed from the priority queue with neighbor $v$ added to it, $dist(v) < dist(u)$.

\begin{algorithm}[H]
\caption{Dijkstra's algorithm}
\begin{algorithmic}

\ForAll{vertices $v$}
    \State $dist(v) \gets \infty$
    \State $prev(v) \gets -1$
    \State $visited(v) \gets false$
\EndFor

\State $dist(src) \gets 0$
\State $pq \gets$ priority queue
\State add $src$ to $pq$ with key $0$

\While{$pq$ is not empty}
    \State $u \gets u$ in $pq$ with minimum $dist(u)$
    
    \If{$visited(v)$}
    \Comment{only remove each node once}
        \State \textbf{continue}
    \EndIf
    \State $visited(v) \gets true$

    \ForAll{neighbors $v$ of $u$}
    \Comment{relax edges}
        %\If{$visited(v)$}
        %    \State \textbf{continue}
        %\EndIf
        \State $alt \gets dist(u) + weight(u, v)$
        \If{not $visited(v)$ \textbf{and} $alt < dist(v)$}
            \State $dist(v) \gets alt$
            \State $prev(v) \gets u$
            \State add $v$ to $pq$ with key $dist(v)$
            \Comment add instead of update-key
        \EndIf
    \EndFor
\EndWhile

\end{algorithmic}
\end{algorithm}


\subsection{Complexity}

Suppose edge $e$ connects vertices $u$ and $v$. If $dist(v) > dist(u) + weight(e)$, then we define the process of \textit{relaxing} an edge $e$ as $dist(v) \gets dist(u) + weight(e)$.

In the worst case, the algorithm will check every edge, and every edge will be relaxed. Each relaxation requires a priority queue insertion operation of complexity $O(\log V)$. Therefore, the complexity of Djikstra's algorithm is $O(E \log V)$.

