\chapter{Graph Theory}

In Chapter 2, we discussed how we can represent the search space of a recursive, brute-force algorithm as a tree of states. This means that all recursive, brute-force algorithms basically perform the same thing: visit each child state and recur. In other words, we learned one way in which we can attack a specific problem using a nonspecific approach. In fact, many seemingly specific problems can be approached by using a variant of a generic algorithm. For these reasons, it is often useful to abstract specific problems into more general, mathematical objects, such as graphs. 


\section{Definitions and Terminology}

Formally, a \textit{graph} consists of a set of \textit{vertices}, or nodes, and a set of \textit{edges}, where each edge connects two vertices. An \textit{undirected} graph is one in which every edge is bidirectional. In other words, if an edge connects vertex $A$ to vertex $B$, then it also connects $B$ to $A$. In contrast, an \textit{directed} graph is one in which every edge points in a single direction. Directed graphs are also commonly called \textit{Digraphs}. Note, however, that it is possible for two directed edges to exist between two vertices. In a \textit{weighted} graph, each edge is associated with some weight. 

Graphs can take on many unusual structures. For example, an edge is allowed to point from a vertex to itself. Graphs may also contain multiple edges between the same pair of vertices. For the sake of simplicity, we will only deal with graphs that contain no self loops and multiple edges in this chapter. This is because these cases usually serve very little purpose in solving actual problems. 

A \textit{path} is a sequence of edges which connected a sequence of vertices.  If the first and last vertices in the sequence are the same, then we refer to the path as a \textit{cycle}. For example, in the digraph below, $(E, F, C, H, C, G)$ is a valid path and $(H, E, F, A)$ is a valid cycle. Two vertices are said to be \textit{connected} if these exists a path between them. A connected graph is a graph in which all pairs of vertices are connected. 

A \textit{tree} is a special kind of undirected graph that contains no cycles. Trees are particularly useful because we can choose a single node to be the \textit{root} of the tree. Then all edges in the tree are given a natural direction, namely towards the root. We will discuss trees more in-depth in a later chapter.

\begin{center}
\begin{minipage}{0.48\linewidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=60mm/#1}]
\draw (0, 0) node [vertex] (n1) {A};
\draw (2, 0) node [vertex] (n2) {B};
\draw (4, 0) node [vertex] (n3) {C};
\draw (6, 0) node [vertex] (n4) {D};
\draw (0, 2) node [vertex] (m1) {E};
\draw (2, 2) node [vertex] (m2) {F};
\draw (4, 2) node [vertex] (m3) {G};
\draw (6, 2) node [vertex] (m4) {H};
\draw[->] (m1) -- (m2);
\draw[->] (m2) -- (n1);
\draw[->] (n1) -- (m1);
\draw[->] (n2) edge [bend left] (m2);
\draw[->] (m2) edge [bend left] (n2);
\draw[->] (n2) -- (n3);
\draw[->] (m2) -- (m3);
\draw[->] (m2) -- (n3);
\draw[->] (n3) -- (m3);
\draw[->] (n3) edge [bend left] (m4);
\draw[->] (m4) edge [bend left] (n3);
\draw[->] (n4) -- (m4);
\end{tikzpicture}
\end{minipage}
\end{center}


\begin{comment}
Roughly speaking, a {\bf graph} is a network of ``cities'' and ``roads.''
Formally speaking, a graph is a collection of points called {\bf nodes} and segments that connect pairs of nodes called {\bf edges}.

In computer science, graphs can be used to represent any abstract objects and their relations.
Graph theory is heavy in vocabulary, but we will just go over the most important terms.

In a {\bf undirected graph}, the edges are bidirectional, aka if one edge connects from node $A$ to node $B$, then it also connects from node $B$ to node $A$.
Then in a {\bf directed graph}, the edges have a preferred direction that they are ``one-way roads.''

A {\bf path} is sequence of nodes that are connected by distinct edges. 
Intuitively speaking, a path is a route we drive from one city to another through roads.
If a path has the same starting and ending nodes, then it is called a {\bf cycle}.
A {\bf tree} is a graph which contains no cycle.

A node is said to be {\bf reachable} from another node if there is a path between them.
A {\bf complete graph} or {\bf clique} is a graph in which each node is reachable from every other node.
\end{comment}



\section{Representations of Graph}

Before discussing graph algorithms, it is important to understand how to represent a graph so that we have access information efficiently. In this section, we will go over the two main graph representations and their respective advantages.

For a graph with $V$ vertices, an \textit{adjacency matrix} is an $V$ by $V$ matrix $A$, where $A_{ij}$ (the entry in row $i$ and column $j$) represents the edge weight between vertices $i$ and $j$. It no edge exists between $i$ and $j$, then $A_{ij} = 0$. In an unweighted graph, we usually represent all of the edge weights as $1$. Note that in an undirected graph, $A$ would be symmetric about its main diagonal. Below is the adjacency matrix for the graph shown above. 

\begin{center}
\begin{minipage}{0.45\linewidth}
\begin{tabular}{c c c c c c c c c}
         & A & B & C & D & E & F & G & H \\
A        & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
B        & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
C        & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
D        & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
E        & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
F        & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\
G        & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
H        & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}
\end{minipage}
\end{center}

An adjacency matrix allows us to determine the edge weight or existence of an edge between two vertices in $O(1)$. Furthermore, we can get all neighbors of any vertex in $O(V)$ by looping across a row of the adjacency matrix. However, The adjacency matrix requires $O(V^2)$ space, which may be undesirable if the number of vertices is large (say, around $V = 2000$). This is also true when the graph is \textit{sparse}, or as relatively few edges. 

We can also represent a graph as a list of sets, where each set contains the neighbors of an associated vertex. In an weighted graph, the sets contains pairs of vertices and their corresponding edge weights. Adjacency lists are less straightforward to implement than adjacency matrices; they are usually implemented as an array of dynamically allocated lists (which are usually provided in the standard library). Below is an adjacency list for the graph shown above. 

\begin{center}
\begin{minipage}{0.45\linewidth}
\begin{tabular}{ c|c c c c}
A        & E \\
B        & C & F \\
C        & G & H \\
D        & H \\
E        & F \\
F        & A & B & C & G \\
G        & \\
H        & C \\
\end{tabular}
\end{minipage}
\end{center}

An adjacency list usually scales better for large $N$ than an adjacency matrix because it requires $O(E)$ space complexity. In the worst case, we can also access all neighbors of any vertex in $O(V)$. The main disadvantage of an adjacency list, however, is that we cannot check the edge between any two vertices in $O(1)$. Fortunately, many algorithms do not requires constant time edge look-up, making adjacency lists an excellent choice for balancing both time and space complexity. 


\section{Graph Search Algorithms}

In this section, we will apply what we've just discussed to learn a number of well-known graph algorithms. 


\subsection{Tree traversal}

In section \ref{recursive-search} we talked about using recursion to explore states, specifically in the case of generating permutations. We also represented those states as a search tree.

In what order did we traverse that tree? We started at the root node, or level 0. Then we chose a node at level 1, and then another at level 2, and so on until we reached the bottom of the tree. At that point, we've reached one node on each of the levels, but no more. Only after that will we start to check other nodes on the same levels.

Why does our recursive method behave like this? Let's say that every time we make a recursive call, we're visiting a node. So when we visit a permutation X, if we are able to we immediately generate a new permutation, we do so, and we visit the new permutation through a recursive call. Only after we get back to the original call do we generate other permutations. Thus, our use of recursion ensures that we examine the deeper elements first, which is why we call this \textit{depth first search}.

\subsection{Depth First Search}

We can use this same recursive strategy to traverse all nodes in a graph. We can treat each node as a possible state, and recur on each of its neighbors.

One change that we must make, however, is to check whether we've already visited a node before recurring on it. This is because unlike trees, graphs can contain cycles, so there may be multiple ways to reach a node. If we don't do this, we might recurse indefinitely along a cycle.

The only difference between an \textit{undirected graph} and a \textit{tree} is that a tree contains no cycles. In other words, it is \textbf{acyclic}. We can express any acyclic undirected graph as a tree if we pick an arbitrary node to be the root, then treat its neighbors as its children, and so on.

We can apply this to a graph represented by an adjacency list, which stores the neighbors of each node. We can also apply this to a grid, in which the neighbors of a cell are the four adjacent cells. Interestingly, the edges that were traversed in the graph correspond to the search tree of the recursion. Assuming the graph is connected, the search tree is a spanning tree of the graph, which means that it includes all of the vertices in the graph.

% insert pseudocode here?


\subsection{Breadth First Search}

Another way to explore the tree is to go level by level. We start at the root node at level 0. Next, we examine its children on level 1. Then, we examine the children of the nodes on level 1, which are on level 2, and so on. How can we implement this in our code?

% maybe math notation is not that good
We could start by forming a list $A_0$ of nodes on level 0; i.e., only the root node. Then, we can find $A_1$ by adding all the children of the nodes in $A_0$, and so on, each time generating $A_{k+1}$ from $A_k$.

In fact, we can do this with only one array, $Q = A_0 + A_1 + ... + A_n$. While we're processing $A_k$, we can simply append the elements of $A_k$ to the end of the list. What does this remind us of? That's right, a queue! For each node we remove from the front of the queue, we insert the children of that node into the back of the queue. This ensures that we process all nodes in the graph in breadth-first order.

% insert pseudocode here?



\section{Shortest Paths}

The shortest-paths problem comes up a lot in competitive programming. Given two nodes $u$ and $v$ in a directed graph, we want to find a path between $u$ and $v$ such that the sum of the edge weights of the path is minimized.

\begin{center}
\begin{tikzpicture}[very thick,edge from parent/.style={draw,<-},level/.style={sibling distance=30mm/#1},scale=0.8]
\draw (0, 1) node [vertex] (v1) {1};
\draw (2.5, 2) node [vertex] (v2) {2};
\draw (4, -1) node [vertex] (v3) {3};
\draw (6, 2) node [vertex] (v4) {4};
\draw (8, 0) node [vertex] (v5) {5};
\draw[->] (v1) -- (v2) node[midway, above left] {4};
\draw[->] (v2) -- (v3) node[midway, above right] {2};
\draw[->] (v1) -- (v3) node[midway, below] {7};
\draw[->] (v2) -- (v4) node[midway, above] {6};
\draw[->] (v3) -- (v4) node[midway, right] {3};
\draw[->] (v3) -- (v5) node[midway, below] {5};
\draw[->] (v4) -- (v5) node[midway, above right] {4};
\end{tikzpicture}
\end{center}


\subsection{Floyd-Warshall}

The Floyd-Warshall algorithm solves the multi-source shortest paths problem; that is, it solves the shortest path problem for every pair of vertices. We use a matrix of distances $dist$, which stores the shortest distance we have found so far for each pair of vertices.

Then, for every vertex $k$, and every pair of vertices $i \rightarrow j$, we try to see if $dist(i,j)$ can be improved by going through $k$. In other words, if the current best distance from $i \rightarrow k \rightarrow j$ is shorter than the current best distance for $i \rightarrow j$ (which could be $\infty$). If it is, we update $dist(i,j)$, which we want to be as short as possible. This is similar to the \textit{relax} operation that we will use for Dijkstra's algorithm, though it may not necessarily involve edges in the original graph.

\begin{algorithm}[H]
\caption{Floyd-Warshall}
\begin{algorithmic}
\State $dist(i, j) \gets \infty$ for vertices $i, j$
\ForAll{vertices $i$}
    \State $dist(i,i) \gets 0$
\EndFor
\ForAll{edges $(u,v)$}
	\State $dist(u,v) \gets weight(u,v)$
\EndFor
\ForAll{vertices $k$}
	\ForAll{vertices $i$}
    	\ForAll{vertices $j$}
        	\If{$dist(i,j) > dist(i,k)+dist(k,j)$}
            	\State $dist(i,j) \gets dist(i,k)+dist(k,j)$
            \EndIf
        \EndFor
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

One way to see why this works is to consider two vertices $u$ and $v$ for which we already know the shortest path from $u \rightarrow v$. Once we consider each of the vertices in that shortest path, $dist(u,v)$ will be the length of the shortest path.

\subsection{Dijkstra's algorithm}
Dijkstra's algorithm calculates the shortest path from a source vertex $u$ to every other node in the graph. If we're only interested in the shortest path from $u \rightarrow v$, we can stop once we remove $v$ from the priority queue.

But why does it work? Every time we remove a vertex $u$ from $pq$, $dist(u)$ is monotonically increasing. This is because the priority queue always gives the minimum element, and for any $u$ removed from the priority queue with neighbor $v$ added to it, $dist(v) < dist(u)$.

\begin{algorithm}[H]
\caption{Dijkstra's algorithm}
\begin{algorithmic}

\ForAll{vertices $v$}
    \State $dist(v) \gets \infty$
    \State $prev(v) \gets -1$
    \State $visited(v) \gets false$
\EndFor

\State $dist(src) \gets 0$
\State $pq \gets$ priority queue
\State add $src$ to $pq$ with key $0$

\While{$pq$ is not empty}
    \State $u \gets u$ in $pq$ with minimum $dist(u)$
    
    \If{$visited(v)$}
    \Comment{only remove each node once}
        \State \textbf{continue}
    \EndIf
    \State $visited(v) \gets true$

    \ForAll{neighbors $v$ of $u$}
    \Comment{relax edges}
        %\If{$visited(v)$}
        %    \State \textbf{continue}
        %\EndIf
        \State $alt \gets dist(u) + weight(u, v)$
        \If{not $visited(v)$ \textbf{and} $alt < dist(v)$}
            \State $dist(v) \gets alt$
            \State $prev(v) \gets u$
            \State add $v$ to $pq$ with key $dist(v)$
            \Comment add instead of update-key
        \EndIf
    \EndFor
\EndWhile

\end{algorithmic}
\end{algorithm}

An important implementation note: C++ implements a \textbf{max heap} rather than a min heap. You can get around this by negating the values of $dist$, or by using \verb|std::greater| as the comparator.


\subsection{Complexity}

Suppose edge $e$ connects vertices $u$ and $v$. If $dist(v) > dist(u) + weight(e)$, then we define the process of \textit{relaxing} an edge $e$ as $dist(v) \gets dist(u) + weight(e)$.

In the worst case, the algorithm will check every edge, and every edge will be relaxed. Each relaxation requires a priority queue insertion operation of complexity $O(\log V)$. Therefore, the complexity of Djikstra's Algorithm is $O(E \log V)$.

\subsection{Bellman-Ford}
If a graph contains negative edge weights, Dijkstra's algorithm cannot be used, because it is a greedy algorithm. Instead, we can use the \textbf{Bellman-Ford} algorithm, which simply relaxes all $E$ edges, $V-1$ times, which is the maximum path length.

If the graph contains a negative cycle, there is no "shortest" path. However, the Bellman-Ford algorithm is useful for detecting such negative cycles. To do this, we check if any distance needs to be updated after running the loop $V-1$ times. If so, there is a path of length $V$, which is impossible without a negative cycle.

\begin{algorithm}[H]
\caption{Bellman-Ford}
\begin{algorithmic}
\ForAll{vertices $v$}
	\State $dist(v) \gets \infty$
    \State $prev(v) \gets -1$
\EndFor
\State $dist(src) \gets 0$
\For{$i = 1,V-1$}
	\ForAll{edges $(u,v)$}
    \Comment{relax every edge}
		\If{$dist(u)+weight(u,v) < dist(v)$}
    	    \State $dist(v) \gets dist(u)+weight(u,v)$
	        \State $prev(v) \gets u$
        \EndIf
	\EndFor
\EndFor
\ForAll{edges $(u,v)$}
	\Comment{check for negative cycles}
	\If{$dist(u)+weight(u,v) < dist(v)$}
   	    \State{negative cycle detected}
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

The complexity of such an algorithm is obviously $O(E V)$. This can be improved by maintaining a queue of vertices whose distances were updated. Although the worst-case complexity remains the same, the average number of iterations decreases to $E$. However, checking for negative cycles will instead require us to look for cycles in the shortest-path tree\footnote{Well, if there is a cycle, it's not a tree...} defined by $prev$.


